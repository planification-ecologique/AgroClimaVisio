"""
Module pour charger et interroger les donn√©es climatiques via DuckDB
Optimis√© pour acc√®s rapide point par point (carr√© de grille)
"""

try:
    import duckdb
    import xarray as xr
    import numpy as np
    import pandas as pd
    import netCDF4 as nc
    from pathlib import Path
    from typing import Optional, Dict, List, Tuple
    from datetime import date, datetime
    DUCKDB_AVAILABLE = True
    NETCDF4_AVAILABLE = True
except ImportError:
    DUCKDB_AVAILABLE = False
    NETCDF4_AVAILABLE = False
    duckdb = None
    xr = None
    np = None
    pd = None
    nc = None

import logging
from models import VariableType, ExperimentType

logger = logging.getLogger(__name__)


class DuckDBClimateLoader:
    """
    Chargeur de donn√©es climatiques utilisant DuckDB pour acc√®s rapide.
    Optimis√© pour requ√™tes point par point (carr√© de grille).
    """
    
    def __init__(self, db_path: Optional[str] = None, data_directory: Optional[str] = None):
        """
        Initialise le chargeur DuckDB.
        
        Args:
            db_path: Chemin vers le fichier DuckDB (cr√©√© si n'existe pas)
            data_directory: R√©pertoire contenant les fichiers NetCDF sources
        """
        if not DUCKDB_AVAILABLE:
            raise ImportError(
                "DuckDB n'est pas install√©. Installez-le avec: "
                "poetry add duckdb"
            )
        
        self.db_path = Path(db_path) if db_path else Path("climate_data.duckdb")
        self.data_directory = Path(data_directory) if data_directory else None
        
        # Connexion DuckDB avec gestion d'erreurs pour les verrous
        try:
            self.conn = duckdb.connect(str(self.db_path))
        except Exception as e:
            if "lock" in str(e).lower() or "conflicting" in str(e).lower():
                raise IOError(
                    f"Le fichier DuckDB est verrouill√© par un autre processus.\n"
                    f"V√©rifiez qu'aucune autre instance du script ne tourne.\n"
                    f"Vous pouvez tuer le processus avec: kill <PID>\n"
                    f"Erreur originale: {e}"
                )
            raise
        
        # Cr√©er le sch√©ma si n√©cessaire
        self._create_schema()
    
    def _create_schema(self):
        """Cr√©e le sch√©ma de la base de donn√©es si n√©cessaire"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS climate_data (
                variable VARCHAR NOT NULL,
                experiment VARCHAR NOT NULL,
                gcm VARCHAR NOT NULL,
                rcm VARCHAR NOT NULL,
                member VARCHAR NOT NULL,
                lat DOUBLE NOT NULL,
                lon DOUBLE NOT NULL,
                time DATE NOT NULL,
                value DOUBLE NOT NULL
            );
        """)
        
        # Cr√©er les index pour performance
        try:
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_spatial ON climate_data(lat, lon);")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_temporal ON climate_data(time);")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_variable ON climate_data(variable, experiment, gcm, rcm);")
        except Exception as e:
            # Les index peuvent d√©j√† exister
            logger.debug(f"Index creation: {e}")
    
    def import_netcdf_file(
        self,
        file_path: str,
        variable: VariableType,
        experiment: ExperimentType,
        gcm: str,
        rcm: str,
        member: str = "r1",
        chunk_size: int = 5000  # R√©duit pour √©conomiser la m√©moire
    ) -> int:
        """
        Importe un fichier NetCDF dans DuckDB de mani√®re optimis√©e en m√©moire.
        Traite les donn√©es par chunks temporels pour √©viter de charger tout en m√©moire.
        
        Args:
            file_path: Chemin vers le fichier NetCDF
            variable: Variable climatique
            experiment: Sc√©nario climatique
            gcm: Mod√®le climatique global
            rcm: Mod√®le climatique r√©gional
            member: Membre d'ensemble
            chunk_size: Nombre de lignes √† ins√©rer par batch dans DuckDB
            time_chunk_size: Nombre de pas de temps √† traiter √† la fois
        
        Returns:
            Nombre de lignes import√©es
        """
        if not DUCKDB_AVAILABLE or not xr:
            raise ImportError("xarray et duckdb doivent √™tre install√©s")
        
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"Fichier non trouv√©: {file_path}")
        
        logger.info(f"Importation de {file_path} dans DuckDB (mode optimis√© m√©moire)...")
        print(f"   üìÇ Ouverture du fichier NetCDF avec netCDF4 (acc√®s direct aux slices)...")
        
        # Utiliser netCDF4 directement pour un acc√®s plus efficace aux slices
        # Cela √©vite l'overhead de xarray et permet un acc√®s direct optimis√©
        if NETCDF4_AVAILABLE:
            nc_file = nc.Dataset(file_path, 'r')
            print(f"   ‚úÖ Fichier ouvert avec netCDF4")
        else:
            # Fallback sur xarray si netCDF4 n'est pas disponible
            ds = xr.open_dataset(file_path)
            print(f"   ‚úÖ Fichier ouvert avec xarray")
            nc_file = None
        
        # Trouver la variable dans le dataset
        var_names = {
            VariableType.PR: ["prAdjust", "prAdjusted", "pr"],
            VariableType.TAS: ["tasAdjust", "tasAdjusted", "tas"],
            VariableType.TASMAX: ["tasmaxAdjust", "tasmaxAdjusted", "tasmax"],
            VariableType.TASMIN: ["tasminAdjust", "tasminAdjusted", "tasmin"],
            VariableType.RSDS: ["rsdsAdjust", "rsdsAdjusted", "rsds"],
            VariableType.RLDS: ["rldsAdjust", "rldsAdjusted", "rlds"],
            VariableType.HUSS: ["hussAdjust", "hussAdjusted", "huss"],
            VariableType.SFCWIND: ["sfcWindAdjust", "sfcWindAdjusted", "sfcWind"]
        }
        
        var_name = None
        if nc_file:
            # Utiliser netCDF4 directement
            for name in var_names.get(variable, [variable.value]):
                if name in nc_file.variables:
                    var_name = name
                    break
            
            if not var_name:
                nc_file.close()
                raise ValueError(f"Variable {variable.value} non trouv√©e dans {file_path}")
            
            print(f"   üîç Variable trouv√©e: {var_name}")
            nc_var = nc_file.variables[var_name]
            nc_time = nc_file.variables['time']
            nc_lat = nc_file.variables['lat']
            nc_lon = nc_file.variables['lon']
            
            # Obtenir les dimensions directement depuis netCDF4
            print(f"   üìä Lecture des dimensions...")
            time_coords = nc_time[:]
            lat_coords_raw = nc_lat[:]
            lon_coords_raw = nc_lon[:]
            
            # Obtenir la shape de la variable
            var_shape = nc_var.shape
            print(f"   üìê Shape de la variable: {var_shape}")
        else:
            # Fallback sur xarray
            for name in var_names.get(variable, [variable.value]):
                if name in ds.data_vars:
                    var_name = name
                    break
            
            if not var_name:
                raise ValueError(f"Variable {variable.value} non trouv√©e dans {file_path}")
            
            print(f"   üîç Variable trouv√©e: {var_name}")
            data_array = ds[var_name]
            
            if 'lat' not in data_array.coords or 'lon' not in data_array.coords:
                raise ValueError("Coordonn√©es 'lat' et 'lon' non trouv√©es dans le dataset")
            
            print(f"   üìä Lecture des dimensions...")
            time_coords = data_array.coords['time'].values
            lat_coords_raw = data_array.coords['lat'].values
            lon_coords_raw = data_array.coords['lon'].values
            var_shape = data_array.shape
            nc_var = None
        
        # V√©rifier si les coordonn√©es sont 1D ou 2D
        lat_shape = lat_coords_raw.shape if hasattr(lat_coords_raw, 'shape') else None
        lon_shape = lon_coords_raw.shape if hasattr(lon_coords_raw, 'shape') else None
        
        logger.info(f"Forme des coordonn√©es lat: {lat_shape}, lon: {lon_shape}")
        print(f"   üìê Forme des coordonn√©es lat: {lat_shape}, lon: {lon_shape}")
        
        # Obtenir les dimensions de la grille
        # Si les coordonn√©es sont 2D, utiliser les dimensions de la grille
        # Sinon, utiliser les dimensions des coordonn√©es 1D
        if lat_shape and len(lat_shape) == 2:
            # Grille 2D: les coordonn√©es sont des arrays 2D
            n_lats, n_lons = lat_shape
            logger.info(f"Grille 2D d√©tect√©e: {n_lats} √ó {n_lons}")
        else:
            # Coordonn√©es 1D: dimensions s√©par√©es
            n_lats = len(lat_coords_raw) if hasattr(lat_coords_raw, '__len__') else 1
            n_lons = len(lon_coords_raw) if hasattr(lon_coords_raw, '__len__') else 1
            logger.info(f"Coordonn√©es 1D d√©tect√©es: {n_lats} lat √ó {n_lons} lon")
        
        n_times = len(time_coords)
        
        def to_float(val):
            """Convertit une valeur en float Python"""
            if isinstance(val, (int, float)):
                return float(val)
            if isinstance(val, (np.integer, np.floating)):
                return float(val)
            if hasattr(val, 'shape') and val.shape == ():
                return float(val.item())
            if hasattr(val, '__len__') and len(val) == 1:
                return float(val[0])
            return float(val)
        
        logger.info(f"Dimensions: {n_times} temps √ó {n_lats} lat √ó {n_lons} lon = {n_times * n_lats * n_lons:,} points")
        print(f"   üìè Dimensions: {n_times} temps √ó {n_lats} lat √ó {n_lons} lon = {n_times * n_lats * n_lons:,} points")
        
        # Pr√©-calculer les coordonn√©es si grille 2D (une seule fois au d√©but)
        lat_coords_2d = None
        lon_coords_2d = None
        if lat_shape and len(lat_shape) == 2:
            print(f"   üîÑ Pr√©-calcul des coordonn√©es 2D ({n_lats} √ó {n_lons} = {n_lats * n_lons:,} points)...")
            # Convertir toutes les coordonn√©es une seule fois pour √©viter les conversions r√©p√©t√©es
            lat_coords_2d = np.array([[to_float(lat_coords_raw[i, j]) for j in range(n_lons)] for i in range(n_lats)])
            lon_coords_2d = np.array([[to_float(lon_coords_raw[i, j]) for j in range(n_lons)] for i in range(n_lats)])
            print(f"   ‚úÖ Coordonn√©es pr√©-calcul√©es")
        
        print(f"   üöÄ D√©but de l'importation...")
        
        total_rows = 0
        rows_buffer = []
        
        # Traiter pas de temps par pas de temps pour minimiser la m√©moire
        for t_idx, time_val in enumerate(time_coords):
            if t_idx % 100 == 0 or t_idx == 0:
                logger.info(f"Traitement du pas de temps {t_idx+1}/{n_times}...")
                print(f"   ‚è≥ Traitement du pas de temps {t_idx+1}/{n_times}...")
            
            # Charger seulement UN pas de temps √† la fois
            # Utiliser netCDF4 pour un acc√®s direct optimis√© au slice
            time_slice = None
            if nc_var is not None:
                # Acc√®s direct avec netCDF4 - beaucoup plus rapide que xarray
                # Lire directement le slice [t_idx, :, :] sans overhead
                # Selon https://annefou.github.io/metos_python/07-LargeFiles/, 
                # netCDF4 permet un acc√®s direct aux slices sans charger tout le fichier
                values_2d = nc_var[t_idx, :, :]  # Shape: (lat, lon)
                # G√©rer les valeurs masqu√©es (masked arrays) en les convertissant en NaN
                if hasattr(values_2d, 'mask'):
                    # Si c'est un masked array, convertir en array numpy normal avec NaN
                    values_2d = np.ma.filled(values_2d, np.nan)
            else:
                # Fallback sur xarray
                time_slice = data_array.isel(time=t_idx)
                values_2d = time_slice.load().values  # Shape: (lat, lon) ou (y, x)
            
            # Convertir la date une seule fois
            if hasattr(time_val, 'date'):
                time_date = time_val.date()
            elif hasattr(time_val, 'item'):
                time_date = pd.to_datetime(time_val.item()).date()
            else:
                time_date = pd.to_datetime(time_val).date()
            
            # It√©rer sur les indices de la grille
            for lat_idx in range(n_lats):
                for lon_idx in range(n_lons):
                    value = float(values_2d[lat_idx, lon_idx])
                    
                    # Ignorer les NaN
                    if np.isnan(value):
                        continue
                    
                    # Extraire les coordonn√©es lat/lon pour ce point de grille
                    if lat_coords_2d is not None:
                        # Grille 2D: utiliser les coordonn√©es pr√©-calcul√©es
                        lat_val = float(lat_coords_2d[lat_idx, lon_idx])
                        lon_val = float(lon_coords_2d[lat_idx, lon_idx])
                    else:
                        # Coordonn√©es 1D: utiliser les indices directement
                        lat_val = to_float(lat_coords_raw[lat_idx])
                        lon_val = to_float(lon_coords_raw[lon_idx])
                    
                    rows_buffer.append({
                        'variable': variable.value,
                        'experiment': experiment.value,
                        'gcm': gcm,
                        'rcm': rcm,
                        'member': member,
                        'lat': lat_val,
                        'lon': lon_val,
                        'time': time_date,
                        'value': value
                    })
                    
                    # Ins√©rer par batch pour √©viter d'accumuler trop en m√©moire
                    if len(rows_buffer) >= chunk_size:
                        df_chunk = pd.DataFrame(rows_buffer)
                        # Enregistrer le DataFrame comme table temporaire
                        self.conn.register('temp_chunk', df_chunk)
                        # Ins√©rer depuis la table temporaire
                        self.conn.execute("INSERT INTO climate_data SELECT * FROM temp_chunk")
                        # Nettoyer la table temporaire
                        self.conn.unregister('temp_chunk')
                        total_rows += len(rows_buffer)
                        rows_buffer = []
                        
                        # Afficher progression
                        if total_rows % (chunk_size * 10) == 0:
                            logger.info(f"  Progression: {total_rows:,} lignes import√©es...")
                            print(f"   üíæ {total_rows:,} lignes import√©es dans la base...")
            
            # Lib√©rer la m√©moire apr√®s chaque pas de temps
            if time_slice is not None:
                del time_slice
            del values_2d
        
        # Ins√©rer les derni√®res lignes
        if rows_buffer:
            df_chunk = pd.DataFrame(rows_buffer)
            self.conn.register('temp_chunk', df_chunk)
            self.conn.execute("INSERT INTO climate_data SELECT * FROM temp_chunk")
            self.conn.unregister('temp_chunk')
            total_rows += len(rows_buffer)
        
            logger.info(f"‚úÖ Importation termin√©e: {total_rows:,} lignes")
            print(f"   ‚úÖ Importation termin√©e: {total_rows:,} lignes")
        
        # Fermer proprement les fichiers
        if nc_file:
            nc_file.close()
        elif 'ds' in locals():
            ds.close()
        
        return total_rows
    
    def get_data_for_grid_cell(
        self,
        lat: float,
        lon: float,
        variables: List[VariableType],
        experiment: ExperimentType,
        gcm: str,
        rcm: str,
        member: str = "r1",
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        tolerance: float = 0.05  # Tol√©rance en degr√©s pour trouver le point le plus proche
    ) -> pd.DataFrame:
        """
        R√©cup√®re toutes les donn√©es pour un carr√© de grille donn√©.
        
        Args:
            lat: Latitude du point
            lon: Longitude du point
            variables: Liste des variables √† r√©cup√©rer
            experiment: Sc√©nario climatique
            gcm: Mod√®le climatique global
            rcm: Mod√®le climatique r√©gional
            member: Membre d'ensemble
            start_date: Date de d√©but (optionnel)
            end_date: Date de fin (optionnel)
            tolerance: Tol√©rance en degr√©s pour trouver le point le plus proche
        
        Returns:
            DataFrame avec colonnes: variable, time, value
        """
        var_names = [v.value for v in variables]
        
        query = """
            SELECT 
                variable,
                time,
                value,
                lat,
                lon
            FROM climate_data
            WHERE lat BETWEEN ? AND ?
              AND lon BETWEEN ? AND ?
              AND variable IN ({})
              AND experiment = ?
              AND gcm = ?
              AND rcm = ?
              AND member = ?
        """.format(','.join(['?' for _ in var_names]))
        
        params = [
            lat - tolerance,
            lat + tolerance,
            lon - tolerance,
            lon + tolerance
        ] + var_names + [
            experiment.value,
            gcm,
            rcm,
            member
        ]
        
        if start_date:
            query += " AND time >= ?"
            params.append(start_date)
        
        if end_date:
            query += " AND time <= ?"
            params.append(end_date)
        
        query += " ORDER BY variable, time"
        
        result = self.conn.execute(query, params).df()
        
        # Si plusieurs points dans la tol√©rance, prendre le plus proche
        if len(result) > 0 and len(result.groupby(['variable', 'time'])) > len(result) / len(var_names):
            # Il y a plusieurs points spatiaux, prendre le plus proche
            result['distance'] = np.sqrt(
                (result['lat'] - lat)**2 + (result['lon'] - lon)**2
            )
            result = result.sort_values('distance').groupby(['variable', 'time']).first().reset_index()
            result = result.drop(columns=['distance', 'lat', 'lon'])
        
        return result
    
    def get_aggregated_data(
        self,
        lat: float,
        lon: float,
        variable: VariableType,
        experiment: ExperimentType,
        gcm: str,
        rcm: str,
        member: str = "r1",
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        aggregation: str = "mean"  # 'mean', 'sum', 'min', 'max', 'count'
    ) -> float:
        """
        R√©cup√®re une valeur agr√©g√©e pour un point et une p√©riode.
        
        Args:
            lat: Latitude
            lon: Longitude
            variable: Variable climatique
            experiment: Sc√©nario climatique
            gcm: Mod√®le climatique global
            rcm: Mod√®le climatique r√©gional
            member: Membre d'ensemble
            start_date: Date de d√©but
            end_date: Date de fin
            aggregation: Type d'agr√©gation ('mean', 'sum', 'min', 'max', 'count')
        
        Returns:
            Valeur agr√©g√©e
        """
        agg_func = {
            'mean': 'AVG',
            'sum': 'SUM',
            'min': 'MIN',
            'max': 'MAX',
            'count': 'COUNT'
        }.get(aggregation.lower(), 'AVG')
        
        query = f"""
            SELECT {agg_func}(value) as result
            FROM climate_data
            WHERE ABS(lat - ?) < 0.05
              AND ABS(lon - ?) < 0.05
              AND variable = ?
              AND experiment = ?
              AND gcm = ?
              AND rcm = ?
              AND member = ?
        """
        
        params = [lat, lon, variable.value, experiment.value, gcm, rcm, member]
        
        if start_date:
            query += " AND time >= ?"
            params.append(start_date)
        
        if end_date:
            query += " AND time <= ?"
            params.append(end_date)
        
        result = self.conn.execute(query, params).fetchone()
        return result[0] if result else None
    
    def get_time_series(
        self,
        lat: float,
        lon: float,
        variable: VariableType,
        experiment: ExperimentType,
        gcm: str,
        rcm: str,
        member: str = "r1",
        start_date: Optional[date] = None,
        end_date: Optional[date] = None
    ) -> pd.DataFrame:
        """
        R√©cup√®re une s√©rie temporelle pour un point et une variable.
        
        Returns:
            DataFrame avec colonnes: time, value
        """
        query = """
            SELECT time, value
            FROM climate_data
            WHERE ABS(lat - ?) < 0.05
              AND ABS(lon - ?) < 0.05
              AND variable = ?
              AND experiment = ?
              AND gcm = ?
              AND rcm = ?
              AND member = ?
        """
        
        params = [lat, lon, variable.value, experiment.value, gcm, rcm, member]
        
        if start_date:
            query += " AND time >= ?"
            params.append(start_date)
        
        if end_date:
            query += " AND time <= ?"
            params.append(end_date)
        
        query += " ORDER BY time"
        
        return self.conn.execute(query, params).df()
    
    def close(self):
        """Ferme la connexion DuckDB"""
        if self.conn:
            self.conn.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

